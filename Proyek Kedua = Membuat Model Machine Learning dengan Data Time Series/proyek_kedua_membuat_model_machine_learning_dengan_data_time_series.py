# -*- coding: utf-8 -*-
"""Proyek Kedua : Membuat Model Machine Learning dengan Data Time Series.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tVUzaJQrNZaMW0UK5uqYo8XneUKmdlM8

Herliana Nur Ekawati

NIM 11201038

GRUP M07

dataset: https://www.kaggle.com/datasets/syedjaferk/delhi-weather-dataset
"""

import pandas as pd
df = pd.read_csv('Delhi_Weather_data.csv')
df

df.isnull().values.any()

df.isnull().sum()

df['dt_txt']=pd.to_datetime(df['dt_txt'])
df['temp'].fillna(df['temp'].mean(), inplace=True)
df = df[['dt_txt', 'temp']]
df

# mengubah kolom dt_txt menjadi date dan buat df baru bernama citynew
city = df[['dt_txt', 'temp']].copy()
city['date'] = city['dt_txt'].dt.date
citynew = city.drop('dt_txt', axis=1)
citynew.set_index('date', inplace=True)
citynew.head

citynew.info()

dates = df['dt_txt'].values
temper = df['temp'].values

import matplotlib.pyplot as plt
plt.figure(figsize=(15,10))
plt.plot(dates, temper)
plt.title('Delhi Weather Temperature')
plt.xlabel('Date')
plt.ylabel('Temperature')
plt.show()

import tensorflow as tf
def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
  series = tf.expand_dims(series, axis=-1)
  ds = tf.data.Dataset.from_tensor_slices(series)
  ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
  ds = ds.flat_map(lambda w: w.batch(window_size +1))
  ds = ds.shuffle(shuffle_buffer)
  ds = ds.map(lambda w: (w[:-1], w[-1:]))
  return ds.batch(batch_size).prefetch(1)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test= train_test_split(temper, dates, test_size=0.2, random_state=0, shuffle=False)
print(len(x_train), len(x_test))

# model
data_x_train = windowed_dataset(x_train, window_size=60, batch_size=100, shuffle_buffer=2000)
data_x_test = windowed_dataset(x_test, window_size=60, batch_size=100, shuffle_buffer=2000)
model = tf.keras.models.Sequential([
    tf.keras.layers.LSTM(64, return_sequences=True),
    tf.keras.layers.Dense(30, activation='relu'),
    tf.keras.layers.Dense(1),
    tf.keras.layers.Lambda(lambda x: x * 400)
])

# optimizer learning rate
lr_sch = tf.keras.callbacks.LearningRateScheduler(
    lambda epoch: 1e-8 * 10**(epoch / 20))
optimizer = tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=['mae'])

max = df['temp'].max()
print('Max: ', max)
min = df['temp'].min()
print('Min:', min)

x = (max-min) * (10/100)
print(x)

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')<x):
      print("Training dihentikan karena mae <10% dari skala data")
      self.model.stop_training = True

callbacks = myCallback()

tf.keras.backend.set_floatx('float64')
history = model.fit(data_x_train, epochs=70, 
                    validation_data=data_x_test,
                    callbacks=[callbacks])

import matplotlib.pyplot as plt
plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('MAE')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

# plot loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['train', 'test'], loc='upper right')
plt.show()